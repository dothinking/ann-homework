{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Training loops\n",
    "\n",
    "In addition to using built-in APIs for training & validation, e.g. `model.fit()`, `model.evaluate()`, we can define custom loops from scratch using **eager execution and the `GradientTape` object**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse the model built with functional API\n",
    "inputs = tf.keras.Input(shape=(28*28,)) \n",
    "x = tf.keras.layers.Dense(256, activation='relu')(inputs)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(10)(x)\n",
    "outputs = tf.keras.layers.Softmax()(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 28*28).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 28*28).astype('float32') / 255\n",
    "\n",
    "# batch dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.5)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select metrics to measure the loss and the accuracy\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf.GradientTape to train the model\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Run the forward pass of the layer.\n",
    "        # The operations that the layer applies to its inputs are going to be recorded\n",
    "        # on the GradientTape.\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(inputs, training=True)\n",
    "        # Compute the loss value for this minibatch.\n",
    "        loss = loss_fn(labels, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # compute metrics\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4689747095108032, Training Accuracy: 99.2699966430664\n",
      "Epoch 2, Loss: 1.4685983657836914, Training Accuracy: 99.29833221435547\n",
      "Epoch 3, Loss: 1.4685064554214478, Training Accuracy: 99.3133316040039\n",
      "Epoch 4, Loss: 1.4682337045669556, Training Accuracy: 99.32833099365234\n",
      "Epoch 5, Loss: 1.468082308769226, Training Accuracy: 99.3499984741211\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "   \n",
    "    # Iterate over the batches of the dataset.\n",
    "    for x_batch_train, y_batch_train in train_dataset:        \n",
    "        train_step(x_batch_train, y_batch_train)\n",
    "        test_step(x_batch_train, y_batch_train)\n",
    "\n",
    "    \n",
    "    # display at the end of an epoch\n",
    "    logs = f'Epoch {epoch+1}, Loss: {train_loss.result()}, Training Accuracy: {train_accuracy.result()*100}'\n",
    "    print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4828331470489502, Test Accuracy: 97.88999938964844\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def test_step(inputs, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(inputs, training=False)\n",
    "    t_loss = loss_fn(labels, predictions)\n",
    "    \n",
    "    # compute loss and accuracy metrics\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    \n",
    "test_step(x_test, y_test)\n",
    "print(f'Loss: {test_loss.result()}, Test Accuracy: {test_accuracy.result()*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
