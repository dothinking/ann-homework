{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import mnist, MNIST, NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(MNIST, NN):\n",
    "    \n",
    "    def __init__(self, n_input, n_output, ckpt_dir='ckpt_cnn'):\n",
    "        self.dropout_rate = tf.placeholder(tf.float32)\n",
    "        super().__init__(n_input, n_output, ckpt_dir)\n",
    "        \n",
    "    @staticmethod\n",
    "    def conv2d(x, W, b, stride=1, name=None):\n",
    "        x = tf.nn.conv2d(x, W, strides=[1,stride, stride, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        return tf.nn.relu(x, name=name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def maxpool(x, ksize=2, stride=2, name=None):\n",
    "        return tf.nn.max_pool(x, ksize=[1, ksize, ksize, 1], strides=[1, stride, stride, 1], padding='SAME', name=name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_var(shape, name=None):\n",
    "        return tf.Variable(tf.random_normal(shape, stddev=0.01), name=name)\n",
    "    \n",
    "    def init_weights(self, n_input, n_output):\n",
    "        '''return tuple: (weights, biases)'''\n",
    "        weights = {\n",
    "            'wc1': self.init_var([11, 11, 1, 32], name='wc1'),\n",
    "            'wc2': self.init_var([5, 5, 32, 64], name='wc2'),\n",
    "            'wc3': self.init_var([3, 3, 64, 128], name='wc3'),\n",
    "            'wc4': self.init_var([3, 3, 128, 128], name='wc4'),\n",
    "            'wc5': self.init_var([3, 3, 128, 64], name='wc5'),\n",
    "            'wd1': self.init_var([4*4*64, 625], name='wd1'),\n",
    "            'wd2': self.init_var([625, 625], name='wd2'),\n",
    "            'out': self.init_var([625, n_output], name='out')\n",
    "        }\n",
    "        biases = {\n",
    "            'bc1': self.init_var([32], name='bc1'),\n",
    "            'bc2': self.init_var([64], name='bc2'),\n",
    "            'bc3': self.init_var([128], name='bc3'),\n",
    "            'bc4': self.init_var([128], name='bc4'),\n",
    "            'bc5': self.init_var([64], name='bc5'),\n",
    "            'bd1': self.init_var([625], name='bd1'),\n",
    "            'bd2': self.init_var([625], name='bd2'),\n",
    "            'out': self.init_var([n_output], name='out')\n",
    "        }        \n",
    "        return weights, biases\n",
    "    \n",
    "    def build_model(self, weights, biases):\n",
    "        '''AlexNet'''\n",
    "        # reshape for CNN\n",
    "        sample_input = tf.reshape(self.sample_input, shape=[-1, 28, 28, 1])\n",
    "        \n",
    "        # convolution 1\n",
    "        conv1 = self.conv2d(sample_input, weights['wc1'], biases['bc1'], stride=1, name='conv1')\n",
    "        pool1 = self.maxpool(conv1, ksize=2, stride=2, name='pool1')\n",
    "        \n",
    "        # convolution 2\n",
    "        conv2 = self.conv2d(pool1, weights['wc2'], biases['bc2'], stride=1, name='conv2')\n",
    "        pool2 = self.maxpool(conv2, ksize=2, stride=2, name='pool2')\n",
    "        \n",
    "        # convolution 3\n",
    "        conv3 = self.conv2d(pool2, weights['wc3'], biases['bc3'], stride=1, name='conv3')\n",
    "        \n",
    "        # convolution 4\n",
    "        conv4 = self.conv2d(conv3, weights['wc4'], biases['bc4'], stride=1, name='conv4')\n",
    "        \n",
    "        # convolution 5\n",
    "        conv5 = self.conv2d(conv4, weights['wc5'], biases['bc5'], stride=1, name='conv5')\n",
    "        pool5 = self.maxpool(conv5, ksize=2, stride=2, name='pool5')\n",
    "        \n",
    "        # full layer 1\n",
    "        fc1 = tf.reshape(pool5, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        fc1 = tf.nn.dropout(fc1, rate=self.dropout_rate)\n",
    "        \n",
    "        # full layer 2\n",
    "        fc2 = tf.add(tf.matmul(fc1, weights['wd2']), biases['bd2'])\n",
    "        fc2 = tf.nn.relu(fc2)\n",
    "        fc2 = tf.nn.dropout(fc2, rate=self.dropout_rate)\n",
    "        \n",
    "        # output layer\n",
    "        out = tf.add(tf.matmul(fc2, weights['out']), biases['out'])\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def build_optimizer(self, target):\n",
    "        '''Gradient Descent Optimizer by default'''\n",
    "        return tf.train.RMSPropOptimizer(0.001, 0.9).minimize(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt_cnn\\model-1000\n",
      "Epoch      1, Iter      200: loss = 0.0054   training accuracy = 100.0000%\n",
      "Epoch      2, Iter      400: loss = 0.0020   training accuracy = 100.0000%\n",
      "Epoch      3, Iter      600: loss = 0.0029   training accuracy = 100.0000%\n",
      "Epoch      4, Iter      800: loss = 0.0010   training accuracy = 100.0000%\n",
      "Epoch      5, Iter     1000: loss = 0.0608   training accuracy = 98.5000%\n",
      "Epoch      6, Iter     1200: loss = 0.0311   training accuracy = 98.5000%\n",
      "Epoch      7, Iter     1400: loss = 0.0401   training accuracy = 99.0000%\n",
      "Epoch      8, Iter     1600: loss = 0.0107   training accuracy = 99.5000%\n",
      "Epoch      9, Iter     1800: loss = 0.0165   training accuracy = 99.5000%\n",
      "Epoch     10, Iter     2000: loss = 0.0300   training accuracy = 99.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.983"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = CNN(28*28, 10, 'ckpt_cnn')\n",
    "\n",
    "# initial accuracy\n",
    "# network.test(mnist.test.images, mnist.test.labels)\n",
    "\n",
    "# train\n",
    "network.train(mnist.train.next_batch, batch_size = 200, \n",
    "              epochs = 10, \n",
    "              display_interval = 1,\n",
    "              save_model = False,\n",
    "              restore_model = True,\n",
    "              save_interval = 500,\n",
    "              train_feed_dict = { network.dropout_rate: 0.5 },\n",
    "              test_feed_dict = { network.dropout_rate: 0.0 })\n",
    "\n",
    "# final accuracy\n",
    "network.test(mnist.test.images, mnist.test.labels, feed_dict = { network.dropout_rate: 0.0 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
